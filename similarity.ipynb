{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('fetch_exercise': pipenv)",
   "metadata": {
    "interpreter": {
     "hash": "fc57c922fb3438dd56a5969a3854a441ee2582fa4039fa4381605f9afe3a1add"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Description\n",
    "\n",
    "This challenge will focus on the similarity between two texts. Your objective is to write a program that takes as inputs two texts and uses a metric to determine how similar they are. Documents that are exactly the same should get a score of 1, and documents that don’t have any words in common should get a score of 0. Please use the samples below to develop your application.\n",
    "\n",
    "You will have to make a number of decisions as you develop this solution:\n",
    "\n",
    "- Do you count punctuation or only words?\n",
    "- Which words should matter in the similarity comparison?\n",
    "- Do you care about the ordering of words?\n",
    "- What metric do you use to assign a numerical value to the similarity?\n",
    "- What type of data structures should be used? (Hint: Dictionaries and lists are particularly helpful data structures that can be leveraged to calculate the similarity of two pieces of text.)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Requirements\n",
    "\n",
    "1. The document similarity algorithm does not need to perform well, and you don’t need to account for all edge cases. Focus on having some fun with it and producing code that we can discuss together.\n",
    "1. Use the 3 sample texts provided below to develop your app. Samples 1 and 2 should be more similar than samples 1 and 3.\n",
    "1. You may choose any language you like, but do not import any libraries.\n",
    "1. The code, at a minimum, must run. Please provide clear instructions on how to run it.\n",
    "\n",
    "When complete, please upload your codebase to a public Git repo (GitHub, Bitbucket, etc.) and email us the link. Please double-check this is publicly accessible.\n",
    "Please assume the evaluator does not have prior experience executing programs in your chosen language. Therefore, please include any documentation necessary to accomplish the above requirements."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Do you count punctuation or only words?\n",
    "\n",
    "I only care about words. Documents will be stripped of punctuation and converted to lower case.\n",
    "\n",
    "### Which words should matter in the similarity comparison?\n",
    "\n",
    "I will remove some common stop words, like articles, conjunctions, and some prepositions in order to focus more on the words that provide contextual meaning to each document.\n",
    "\n",
    "### Do you care about the ordering of words?\n",
    "\n",
    "I started with the assumption that word ordering would be important, but the method I ended with (frequency of words in a document) does not take order into account. Including sequences of words as tokens in the way I did decreases the similarty metric because it increases the number of opportunities for the documents to differ.\n",
    "\n",
    "### What metric do you use to assign a numerical value to the similarity?\n",
    "\n",
    "I used cosine similarity of term frequency between each document to calculate the similarity.\n",
    "\n",
    "### What type of data structures should be used?\n",
    "\n",
    "I will use dictionaries, lists, and tuples."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "\n",
    "def clean(doc:str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans a string for processing.\n",
    "    \n",
    "    Removes non-alphanumeric characters and extra whitespace.\n",
    "    Converts to lowercase.\n",
    "\n",
    "    Parameters:\n",
    "    doc (str): String to be cleaned\n",
    "\n",
    "    Returns:\n",
    "    str: Cleaned string\n",
    "    \"\"\"\n",
    "    doc = re.sub(r'[^a-zA-Z0-9 ]', '', doc)  # remove non-alphanumeric or space characters\n",
    "    doc = re.sub(r'\\s+', ' ', doc)  # trim continous whitespace down to 1 space\n",
    "    doc = doc.lower().strip()\n",
    "    return doc\n",
    "\n",
    "STOP_WORDS = {'a', 'an', 'the', 'of', 'for', 'and', 'or'}\n",
    "def tokenize(doc:str) -> list:\n",
    "    \"\"\"\n",
    "    Creates tokenized representation of a document string.\n",
    "\n",
    "    Splits a string on whitespace, ignoring words that are in `STOP_WORDS`.\n",
    "\n",
    "    Parameters:\n",
    "    doc (str): String to be tokenized\n",
    "\n",
    "    Returns:\n",
    "    [str]: list of individual words\n",
    "    \"\"\"\n",
    "    tokens = [word for word in doc.split() if word not in STOP_WORDS]\n",
    "    return tokens\n",
    "\n",
    "def frequency(tokens:list, seq_len:int = 5) -> dict:\n",
    "    \"\"\"\n",
    "    Counts the frequency of unique sequences in a list of tokens.\n",
    "\n",
    "    Returns a dictionary where keys are unique sequences of length 1 to `seq_len`\n",
    "    and values are the count of occurences of those sequences in the `tokens` list.\n",
    "\n",
    "    Parameters:\n",
    "    tokens (list): list of tokens parsed from a document.\n",
    "    seq_len (int): (min 1) max length of sequences to count.\n",
    "\n",
    "    Returns:\n",
    "    dict: {sequence: count}\n",
    "    \"\"\"\n",
    "    assert seq_len >= 1, 'seq_len must be at least 1.'\n",
    "    seq_count = {}\n",
    "\n",
    "    for length in range(1, seq_len + 1):\n",
    "        for i in range(len(tokens) - (length - 1)):\n",
    "            seq = tuple(tokens[i:i+length])\n",
    "            if seq in seq_count:\n",
    "                seq_count[seq] = seq_count[seq] + 1\n",
    "            else:\n",
    "                seq_count[seq] = 1\n",
    "\n",
    "    return seq_count\n",
    "\n",
    "# took these two functions from\n",
    "# https://www.geeksforgeeks.org/measuring-the-document-similarity-in-python/\n",
    "# modified `vector_angle` from above into `cosine_similarity` based on\n",
    "# https://neo4j.com/docs/graph-algorithms/current/labs-algorithms/cosine/\n",
    "# because the results were not making sense to me\n",
    "# (comparing a document to itself produced a score of 0.0)\n",
    "def dot_product(tf1, tf2):\n",
    "    \"\"\"\n",
    "    Calculates the dot product of two term frquency vectors.\n",
    "\n",
    "    Returns the dot product of the frequencies of matching terms\n",
    "    in two term frequency vectors. The term frequency vectors are\n",
    "    dictionaries with the term as the key and the frequency as the value.\n",
    "\n",
    "    Parameters:\n",
    "    tf1 (dict): Term frequency vector 1 {(term, ): frequency}\n",
    "    tf2 (dict): Term frequency vector 2 {(term, ): frequency}\n",
    "\n",
    "    Returns:\n",
    "    int: Dot product of the frequencies of matching terms\n",
    "    \"\"\"\n",
    "    sum = 0.0\n",
    "    for k1 in tf1:\n",
    "        if k1 in tf2:\n",
    "            sum = sum + (tf1[k1] * tf2[k1])\n",
    "    return sum\n",
    "\n",
    "# def vector_angle(tf1, tf2):\n",
    "def cosine_similarity(tf1, tf2):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between two term frequency vectors.\n",
    "\n",
    "    Returns the cosine similarity of two term frequency vectors. The term\n",
    "    frequency vectors are dictionaries with the term as the key and the\n",
    "    frequency as the value.\n",
    "\n",
    "    Parameters:\n",
    "    tf1 (dict): Term frequency vector 1 {(term, ): frequency}\n",
    "    tf2 (dict): Term frequency vector 2 {(term, ): frequency}\n",
    "\n",
    "    Returns:\n",
    "    float: Cosine similarity of two term frequency vectors\n",
    "    \"\"\"\n",
    "    numerator = dot_product(tf1, tf2)\n",
    "    denominator = math.sqrt(dot_product(tf1, tf1)) * math.sqrt(dot_product(tf2, tf2))\n",
    "      \n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(d1, d2):\n",
    "    \"\"\"\n",
    "    Calculate the similarity between two document strings.\n",
    "\n",
    "    Returns the rounded cosine similarity between two documents\n",
    "    after cleaning and tokenizing them.\n",
    "\n",
    "    Parameters:\n",
    "    d1 (str): Document 1\n",
    "    d2 (str): Document 2\n",
    "\n",
    "    Returns:\n",
    "    float: similarity\n",
    "    \"\"\"\n",
    "    # clean and tokenize\n",
    "    d1 = tokenize(clean(d1))\n",
    "    d2 = tokenize(clean(d2))\n",
    "\n",
    "    # build token index so as to not be operating on strings\n",
    "    vocab = list(set(d1 + d2))\n",
    "    v1 = [vocab.index(token) for token in d1]\n",
    "    v2 = [vocab.index(token) for token in d2]\n",
    "\n",
    "    # calculate token frequency\n",
    "    # single tokens gives best results on sample documents\n",
    "    # tested sequences from 1 to 5 length\n",
    "    seq_length = 1\n",
    "    f1 = frequency(v1, seq_length)\n",
    "    f2 = frequency(v2, seq_length)\n",
    "\n",
    "    return round(cosine_similarity(f1, f2), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"The easiest way to earn points with Fetch Rewards is to just shop for the products you already love. If you have any participating brands on your receipt, you'll get points based on the cost of the products. You don't need to clip any coupons or scan individual barcodes. Just scan each grocery receipt after you shop and we'll find the savings for you.\"\n",
    "\n",
    "doc2 = \"The easiest way to earn points with Fetch Rewards is to just shop for the items you already buy. If you have any eligible brands on your receipt, you will get points based on the total cost of the products. You do not need to cut out any coupons or scan individual UPCs. Just scan your receipt after you check out and we will find the savings for you.\"\n",
    "\n",
    "doc3 = \"We are always looking for opportunities for you to earn more points, which is why we also give you a selection of Special Offers. These Special Offers are opportunities to earn bonus points on top of the regular points you earn every time you purchase a participating brand. No need to pre-select these offers, we'll give you the points whether or not you knew about the offer. We just think it is easier that way.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1,2 0.85\n1,3 0.51\n2,3 0.53\n"
     ]
    }
   ],
   "source": [
    "print('1,2', similarity(doc1, doc2))\n",
    "print('1,3', similarity(doc1, doc3))\n",
    "print('2,3', similarity(doc2, doc3))"
   ]
  }
 ]
}